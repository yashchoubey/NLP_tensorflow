{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'positive']\n",
      "10662\n",
      "10662\n"
     ]
    }
   ],
   "source": [
    "trainset = sklearn.datasets.load_files(container_path = '/home/yash/Downloads/to_push/NLP-Models-master/Text-Classification/data/', encoding = 'UTF-8')\n",
    "trainset.data, trainset.target = separate_dataset(trainset,1.0)\n",
    "print (trainset.target_names)\n",
    "print (len(trainset.data))\n",
    "print (len(trainset.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " ...\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "10662\n",
      "[    0     1     2 ... 10659 10660 10661]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ONEHOT = np.zeros((len(trainset.data),len(trainset.target_names)))\n",
    "print ONEHOT\n",
    "print len(trainset.data)\n",
    "print np.arange(len(trainset.data))\n",
    "#print trainset.target\n",
    "\n",
    "ONEHOT[np.arange(len(trainset.data)),trainset.target] = 1.0\n",
    "print ONEHOT\n",
    "train_X, test_X, train_Y, test_Y, train_onehot, test_onehot = train_test_split(trainset.data, trainset.target, ONEHOT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab from size: 20465\n",
      "('Most common words', [(u'the', 10129), (u'a', 7312), (u'and', 6199), (u'of', 6063), (u'to', 4233), (u'is', 3378)])\n",
      "('Sample data', [4, 662, 9, 2543, 8, 22, 4, 3558, 18064, 98], [u'the', u'rock', u'is', u'destined', u'to', u'be', u'the', u'21st', u'centurys', u'new'])\n"
     ]
    }
   ],
   "source": [
    "concat = ' '.join(trainset.data).split()\n",
    "vocabulary_size = len(list(set(concat)))\n",
    "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\n",
    "print('vocab from size: %d'%(vocabulary_size))\n",
    "print('Most common words', count[4:10])\n",
    "print('Sample data', data[:10], [rev_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3\n"
     ]
    }
   ],
   "source": [
    "GO = dictionary['GO']\n",
    "PAD = dictionary['PAD']\n",
    "EOS = dictionary['EOS']\n",
    "UNK = dictionary['UNK']\n",
    "print GO, PAD,EOS ,UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, size_layer, num_layers, embedded_size,\n",
    "                 dict_size, dimension_output, learning_rate):\n",
    "        \n",
    "        def cells(reuse=False):\n",
    "            return tf.nn.rnn_cell.BasicRNNCell(size_layer,reuse=reuse)\n",
    "        \n",
    "        self.X = tf.placeholder(tf.int32, [None, None])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n",
    "        \n",
    "        encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))\n",
    "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
    "        \n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n",
    "        \n",
    "        outputs, _ = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded, dtype = tf.float32)\n",
    "        \n",
    "        W = tf.get_variable('w',shape=(size_layer, dimension_output),initializer=tf.contrib.layers.xavier_initializer())\n",
    "        #W = tf.get_variable('w',shape=(size_layer, dimension_output),initializer=tf.orthogonal_initializer())\n",
    "        b = tf.get_variable('b',shape=(dimension_output),initializer=tf.zeros_initializer())\n",
    "        \n",
    "        self.logits = tf.matmul(outputs[:, -1], W) + b\n",
    "        \n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logits, labels = self.Y))\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        \n",
    "        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_layer = 128\n",
    "num_layers = 4\n",
    "embedded_size = 128\n",
    "dimension_output = len(trainset.target_names)\n",
    "learning_rate = 1e-6\n",
    "maxlen = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(size_layer,num_layers,embedded_size,vocabulary_size+4,dimension_output,learning_rate)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, pass acc: 0.000000, current acc: 0.513672\n",
      "epoch: 0, training loss: 0.779013, training acc: 0.491122, valid loss: 0.759074, valid acc: 0.513672\n",
      "\n",
      "epoch: 1, training loss: 0.773794, training acc: 0.493016, valid loss: 0.755771, valid acc: 0.513184\n",
      "\n",
      "epoch: 2, pass acc: 0.513672, current acc: 0.515137\n",
      "epoch: 2, training loss: 0.769165, training acc: 0.494673, valid loss: 0.752857, valid acc: 0.515137\n",
      "\n",
      "epoch: 3, training loss: 0.764989, training acc: 0.497159, valid loss: 0.750269, valid acc: 0.515137\n",
      "\n",
      "epoch: 4, training loss: 0.761201, training acc: 0.497396, valid loss: 0.747956, valid acc: 0.515137\n",
      "\n",
      "epoch: 5, pass acc: 0.515137, current acc: 0.520508\n",
      "epoch: 5, training loss: 0.757745, training acc: 0.497988, valid loss: 0.745874, valid acc: 0.520508\n",
      "\n",
      "epoch: 6, training loss: 0.754575, training acc: 0.498935, valid loss: 0.743988, valid acc: 0.519531\n",
      "\n",
      "epoch: 7, training loss: 0.751650, training acc: 0.500000, valid loss: 0.742268, valid acc: 0.517578\n",
      "\n",
      "epoch: 8, training loss: 0.748937, training acc: 0.500237, valid loss: 0.740690, valid acc: 0.515625\n",
      "\n",
      "epoch: 9, training loss: 0.746408, training acc: 0.500829, valid loss: 0.739234, valid acc: 0.518555\n",
      "\n",
      "epoch: 10, training loss: 0.744040, training acc: 0.500237, valid loss: 0.737882, valid acc: 0.518555\n",
      "\n",
      "epoch: 11, training loss: 0.741814, training acc: 0.501539, valid loss: 0.736621, valid acc: 0.516602\n",
      "\n",
      "epoch: 12, training loss: 0.739712, training acc: 0.502131, valid loss: 0.735440, valid acc: 0.514160\n",
      "\n",
      "epoch: 13, training loss: 0.737720, training acc: 0.502367, valid loss: 0.734330, valid acc: 0.515137\n",
      "\n",
      "epoch: 14, training loss: 0.735829, training acc: 0.503196, valid loss: 0.733283, valid acc: 0.515137\n",
      "\n",
      "epoch: 15, training loss: 0.734027, training acc: 0.504380, valid loss: 0.732294, valid acc: 0.514648\n",
      "\n",
      "epoch: 16, training loss: 0.732306, training acc: 0.505327, valid loss: 0.731358, valid acc: 0.515625\n",
      "\n",
      "epoch: 17, training loss: 0.730661, training acc: 0.504616, valid loss: 0.730470, valid acc: 0.513672\n",
      "\n",
      "epoch: 18, training loss: 0.729084, training acc: 0.504853, valid loss: 0.729627, valid acc: 0.512207\n",
      "\n",
      "epoch: 19, training loss: 0.727571, training acc: 0.506866, valid loss: 0.728827, valid acc: 0.513672\n",
      "\n",
      "epoch: 20, training loss: 0.726118, training acc: 0.507221, valid loss: 0.728066, valid acc: 0.513672\n",
      "\n",
      "epoch: 21, training loss: 0.724720, training acc: 0.506866, valid loss: 0.727342, valid acc: 0.511719\n",
      "\n",
      "epoch: 22, training loss: 0.723373, training acc: 0.507457, valid loss: 0.726653, valid acc: 0.510254\n",
      "\n",
      "epoch: 23, training loss: 0.722076, training acc: 0.508641, valid loss: 0.725997, valid acc: 0.511230\n",
      "\n",
      "epoch: 24, training loss: 0.720824, training acc: 0.509470, valid loss: 0.725373, valid acc: 0.510742\n",
      "\n",
      "epoch: 25, training loss: 0.719616, training acc: 0.512547, valid loss: 0.724778, valid acc: 0.510254\n",
      "\n",
      "epoch: 26, training loss: 0.718448, training acc: 0.514796, valid loss: 0.724212, valid acc: 0.511230\n",
      "\n",
      "epoch: 27, training loss: 0.717319, training acc: 0.515507, valid loss: 0.723673, valid acc: 0.510742\n",
      "\n",
      "epoch: 28, training loss: 0.716226, training acc: 0.515980, valid loss: 0.723159, valid acc: 0.511719\n",
      "\n",
      "epoch: 29, training loss: 0.715169, training acc: 0.516454, valid loss: 0.722670, valid acc: 0.510742\n",
      "\n",
      "epoch: 30, training loss: 0.714144, training acc: 0.517992, valid loss: 0.722203, valid acc: 0.508789\n",
      "\n",
      "epoch: 31, training loss: 0.713150, training acc: 0.519058, valid loss: 0.721759, valid acc: 0.510742\n",
      "\n",
      "epoch: 32, training loss: 0.712187, training acc: 0.520478, valid loss: 0.721336, valid acc: 0.513184\n",
      "\n",
      "epoch: 33, training loss: 0.711251, training acc: 0.520597, valid loss: 0.720933, valid acc: 0.512207\n",
      "\n",
      "epoch: 34, training loss: 0.710343, training acc: 0.520952, valid loss: 0.720549, valid acc: 0.511230\n",
      "\n",
      "epoch: 35, training loss: 0.709460, training acc: 0.523082, valid loss: 0.720184, valid acc: 0.511230\n",
      "\n",
      "epoch: 36, training loss: 0.708601, training acc: 0.524858, valid loss: 0.719835, valid acc: 0.510254\n",
      "\n",
      "epoch: 37, training loss: 0.707766, training acc: 0.526515, valid loss: 0.719503, valid acc: 0.511719\n",
      "\n",
      "epoch: 38, training loss: 0.706953, training acc: 0.526515, valid loss: 0.719187, valid acc: 0.512207\n",
      "\n",
      "epoch: 39, training loss: 0.706161, training acc: 0.527107, valid loss: 0.718886, valid acc: 0.511230\n",
      "\n",
      "epoch: 40, training loss: 0.705389, training acc: 0.526989, valid loss: 0.718599, valid acc: 0.512695\n",
      "\n",
      "epoch: 41, training loss: 0.704637, training acc: 0.527344, valid loss: 0.718327, valid acc: 0.512695\n",
      "\n",
      "epoch: 42, training loss: 0.703903, training acc: 0.528054, valid loss: 0.718067, valid acc: 0.512207\n",
      "\n",
      "epoch: 43, training loss: 0.703186, training acc: 0.527699, valid loss: 0.717819, valid acc: 0.513184\n",
      "\n",
      "epoch: 44, training loss: 0.702487, training acc: 0.527936, valid loss: 0.717584, valid acc: 0.513184\n",
      "\n",
      "epoch: 45, training loss: 0.701804, training acc: 0.528291, valid loss: 0.717360, valid acc: 0.513184\n",
      "\n",
      "epoch: 46, training loss: 0.701135, training acc: 0.530421, valid loss: 0.717147, valid acc: 0.513184\n",
      "\n",
      "epoch: 47, training loss: 0.700482, training acc: 0.531250, valid loss: 0.716944, valid acc: 0.512695\n",
      "\n",
      "epoch: 48, training loss: 0.699843, training acc: 0.533026, valid loss: 0.716751, valid acc: 0.514160\n",
      "\n",
      "epoch: 49, training loss: 0.699217, training acc: 0.534920, valid loss: 0.716568, valid acc: 0.510742\n",
      "\n",
      "epoch: 50, training loss: 0.698605, training acc: 0.535630, valid loss: 0.716394, valid acc: 0.508789\n",
      "\n",
      "epoch: 51, training loss: 0.698004, training acc: 0.534564, valid loss: 0.716229, valid acc: 0.511719\n",
      "\n",
      "epoch: 52, training loss: 0.697416, training acc: 0.536340, valid loss: 0.716072, valid acc: 0.510742\n",
      "\n",
      "epoch: 53, training loss: 0.696839, training acc: 0.537879, valid loss: 0.715922, valid acc: 0.511230\n",
      "\n",
      "epoch: 54, training loss: 0.696273, training acc: 0.538826, valid loss: 0.715781, valid acc: 0.512207\n",
      "\n",
      "epoch: 55, training loss: 0.695717, training acc: 0.541193, valid loss: 0.715647, valid acc: 0.510742\n",
      "\n",
      "epoch: 56, training loss: 0.695171, training acc: 0.541903, valid loss: 0.715520, valid acc: 0.509766\n",
      "\n",
      "epoch: 57, training loss: 0.694635, training acc: 0.542140, valid loss: 0.715399, valid acc: 0.507324\n",
      "\n",
      "epoch: 58, training loss: 0.694109, training acc: 0.543205, valid loss: 0.715285, valid acc: 0.507324\n",
      "\n",
      "epoch: 59, training loss: 0.693591, training acc: 0.543324, valid loss: 0.715177, valid acc: 0.507812\n",
      "\n",
      "epoch: 60, training loss: 0.693082, training acc: 0.543442, valid loss: 0.715075, valid acc: 0.505859\n",
      "\n",
      "epoch: 61, training loss: 0.692581, training acc: 0.544981, valid loss: 0.714979, valid acc: 0.504395\n",
      "\n",
      "epoch: 62, training loss: 0.692088, training acc: 0.545455, valid loss: 0.714888, valid acc: 0.504395\n",
      "\n",
      "epoch: 63, training loss: 0.691602, training acc: 0.546520, valid loss: 0.714803, valid acc: 0.503418\n",
      "\n",
      "epoch: 64, training loss: 0.691124, training acc: 0.547585, valid loss: 0.714722, valid acc: 0.502930\n",
      "\n",
      "epoch: 65, training loss: 0.690653, training acc: 0.548532, valid loss: 0.714646, valid acc: 0.502441\n",
      "\n",
      "epoch: 66, training loss: 0.690188, training acc: 0.549834, valid loss: 0.714575, valid acc: 0.502930\n",
      "\n",
      "epoch: 67, training loss: 0.689730, training acc: 0.550426, valid loss: 0.714509, valid acc: 0.500977\n",
      "\n",
      "epoch: 68, training loss: 0.689279, training acc: 0.550308, valid loss: 0.714446, valid acc: 0.500000\n",
      "\n",
      "epoch: 69, training loss: 0.688833, training acc: 0.551018, valid loss: 0.714388, valid acc: 0.499512\n",
      "\n",
      "epoch: 70, training loss: 0.688393, training acc: 0.551255, valid loss: 0.714334, valid acc: 0.500000\n",
      "\n",
      "epoch: 71, training loss: 0.687959, training acc: 0.552794, valid loss: 0.714283, valid acc: 0.500000\n",
      "\n",
      "epoch: 72, training loss: 0.687530, training acc: 0.552320, valid loss: 0.714236, valid acc: 0.499512\n",
      "\n",
      "epoch: 73, training loss: 0.687107, training acc: 0.553385, valid loss: 0.714193, valid acc: 0.500000\n",
      "\n",
      "epoch: 74, training loss: 0.686688, training acc: 0.553385, valid loss: 0.714153, valid acc: 0.500488\n",
      "\n",
      "epoch: 75, training loss: 0.686274, training acc: 0.553504, valid loss: 0.714117, valid acc: 0.499023\n",
      "\n",
      "epoch: 76, training loss: 0.685865, training acc: 0.554214, valid loss: 0.714084, valid acc: 0.499512\n",
      "\n",
      "epoch: 77, training loss: 0.685460, training acc: 0.554332, valid loss: 0.714053, valid acc: 0.498535\n",
      "\n",
      "epoch: 78, training loss: 0.685060, training acc: 0.554451, valid loss: 0.714026, valid acc: 0.498535\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 79, training loss: 0.684664, training acc: 0.555516, valid loss: 0.714002, valid acc: 0.499023\n",
      "\n",
      "epoch: 80, training loss: 0.684272, training acc: 0.556226, valid loss: 0.713980, valid acc: 0.499023\n",
      "\n",
      "epoch: 81, training loss: 0.683884, training acc: 0.556108, valid loss: 0.713961, valid acc: 0.499023\n",
      "\n",
      "epoch: 82, training loss: 0.683500, training acc: 0.556937, valid loss: 0.713945, valid acc: 0.499023\n",
      "\n",
      "epoch: 83, training loss: 0.683119, training acc: 0.556463, valid loss: 0.713931, valid acc: 0.499512\n",
      "\n",
      "epoch: 84, training loss: 0.682742, training acc: 0.556108, valid loss: 0.713919, valid acc: 0.499512\n",
      "\n",
      "epoch: 85, training loss: 0.682368, training acc: 0.557410, valid loss: 0.713910, valid acc: 0.499512\n",
      "\n",
      "epoch: 86, training loss: 0.681998, training acc: 0.558120, valid loss: 0.713903, valid acc: 0.499512\n",
      "\n",
      "epoch: 87, training loss: 0.681631, training acc: 0.558949, valid loss: 0.713899, valid acc: 0.500977\n",
      "\n",
      "epoch: 88, training loss: 0.681266, training acc: 0.560251, valid loss: 0.713897, valid acc: 0.501465\n",
      "\n",
      "epoch: 89, training loss: 0.680905, training acc: 0.560961, valid loss: 0.713896, valid acc: 0.500000\n",
      "\n",
      "epoch: 90, training loss: 0.680547, training acc: 0.561435, valid loss: 0.713898, valid acc: 0.500000\n",
      "\n",
      "epoch: 91, training loss: 0.680191, training acc: 0.561435, valid loss: 0.713902, valid acc: 0.500977\n",
      "\n",
      "epoch: 92, training loss: 0.679839, training acc: 0.562500, valid loss: 0.713908, valid acc: 0.500000\n",
      "\n",
      "epoch: 93, training loss: 0.679488, training acc: 0.562973, valid loss: 0.713915, valid acc: 0.502441\n",
      "\n",
      "epoch: 94, training loss: 0.679141, training acc: 0.563447, valid loss: 0.713925, valid acc: 0.504883\n",
      "\n",
      "epoch: 95, training loss: 0.678796, training acc: 0.563920, valid loss: 0.713936, valid acc: 0.505371\n",
      "\n",
      "epoch: 96, training loss: 0.678453, training acc: 0.565104, valid loss: 0.713949, valid acc: 0.504883\n",
      "\n",
      "epoch: 97, training loss: 0.678112, training acc: 0.566288, valid loss: 0.713964, valid acc: 0.503418\n",
      "\n",
      "epoch: 98, training loss: 0.677774, training acc: 0.566170, valid loss: 0.713980, valid acc: 0.502441\n",
      "\n",
      "epoch: 99, training loss: 0.677438, training acc: 0.566288, valid loss: 0.713998, valid acc: 0.503418\n",
      "\n",
      "epoch: 100, training loss: 0.677104, training acc: 0.566998, valid loss: 0.714018, valid acc: 0.504395\n",
      "\n",
      "epoch: 101, training loss: 0.676772, training acc: 0.567708, valid loss: 0.714039, valid acc: 0.504883\n",
      "\n",
      "epoch: 102, training loss: 0.676442, training acc: 0.567827, valid loss: 0.714061, valid acc: 0.505371\n",
      "\n",
      "epoch: 103, training loss: 0.676114, training acc: 0.568774, valid loss: 0.714086, valid acc: 0.503906\n",
      "\n",
      "epoch: 104, training loss: 0.675788, training acc: 0.569484, valid loss: 0.714111, valid acc: 0.502441\n",
      "\n",
      "epoch: 105, training loss: 0.675464, training acc: 0.570431, valid loss: 0.714138, valid acc: 0.503906\n",
      "\n",
      "epoch: 106, training loss: 0.675141, training acc: 0.571023, valid loss: 0.714167, valid acc: 0.503906\n",
      "\n",
      "epoch: 107, training loss: 0.674820, training acc: 0.571259, valid loss: 0.714197, valid acc: 0.503906\n",
      "\n",
      "epoch: 108, training loss: 0.674501, training acc: 0.571733, valid loss: 0.714228, valid acc: 0.504883\n",
      "\n",
      "epoch: 109, training loss: 0.674183, training acc: 0.571851, valid loss: 0.714261, valid acc: 0.505371\n",
      "\n",
      "epoch: 110, training loss: 0.673867, training acc: 0.573509, valid loss: 0.714295, valid acc: 0.504883\n",
      "\n",
      "epoch: 111, training loss: 0.673552, training acc: 0.573864, valid loss: 0.714330, valid acc: 0.504883\n",
      "\n",
      "epoch: 112, training loss: 0.673239, training acc: 0.574811, valid loss: 0.714366, valid acc: 0.504883\n",
      "\n",
      "epoch: 113, training loss: 0.672927, training acc: 0.575758, valid loss: 0.714404, valid acc: 0.503906\n",
      "\n",
      "epoch: 114, training loss: 0.672617, training acc: 0.576113, valid loss: 0.714443, valid acc: 0.504883\n",
      "\n",
      "epoch: 115, training loss: 0.672308, training acc: 0.577296, valid loss: 0.714484, valid acc: 0.505859\n",
      "\n",
      "epoch: 116, training loss: 0.672000, training acc: 0.577770, valid loss: 0.714525, valid acc: 0.505371\n",
      "\n",
      "epoch: 117, training loss: 0.671694, training acc: 0.577888, valid loss: 0.714568, valid acc: 0.505859\n",
      "\n",
      "epoch: 118, training loss: 0.671389, training acc: 0.579309, valid loss: 0.714612, valid acc: 0.505859\n",
      "\n",
      "epoch: 119, training loss: 0.671085, training acc: 0.579427, valid loss: 0.714657, valid acc: 0.507324\n",
      "\n",
      "epoch: 120, training loss: 0.670782, training acc: 0.580611, valid loss: 0.714703, valid acc: 0.507812\n",
      "\n",
      "epoch: 121, training loss: 0.670480, training acc: 0.581439, valid loss: 0.714750, valid acc: 0.507324\n",
      "\n",
      "epoch: 122, training loss: 0.670179, training acc: 0.582386, valid loss: 0.714798, valid acc: 0.507324\n",
      "\n",
      "epoch: 123, training loss: 0.669880, training acc: 0.582860, valid loss: 0.714848, valid acc: 0.507812\n",
      "\n",
      "epoch: 124, training loss: 0.669581, training acc: 0.582978, valid loss: 0.714899, valid acc: 0.508789\n",
      "\n",
      "epoch: 125, training loss: 0.669284, training acc: 0.583807, valid loss: 0.714950, valid acc: 0.508301\n",
      "\n",
      "epoch: 126, training loss: 0.668987, training acc: 0.583570, valid loss: 0.715003, valid acc: 0.509766\n",
      "\n",
      "epoch: 127, training loss: 0.668691, training acc: 0.584635, valid loss: 0.715057, valid acc: 0.509766\n",
      "\n",
      "epoch: 128, training loss: 0.668397, training acc: 0.584991, valid loss: 0.715112, valid acc: 0.508789\n",
      "\n",
      "epoch: 129, training loss: 0.668103, training acc: 0.585701, valid loss: 0.715168, valid acc: 0.508301\n",
      "\n",
      "epoch: 130, training loss: 0.667810, training acc: 0.585582, valid loss: 0.715225, valid acc: 0.508789\n",
      "\n",
      "epoch: 131, training loss: 0.667518, training acc: 0.586648, valid loss: 0.715283, valid acc: 0.507812\n",
      "\n",
      "epoch: 132, training loss: 0.667227, training acc: 0.587358, valid loss: 0.715343, valid acc: 0.507812\n",
      "\n",
      "epoch: 133, training loss: 0.666936, training acc: 0.587713, valid loss: 0.715403, valid acc: 0.507324\n",
      "\n",
      "epoch: 134, training loss: 0.666647, training acc: 0.588187, valid loss: 0.715464, valid acc: 0.507324\n",
      "\n",
      "epoch: 135, training loss: 0.666358, training acc: 0.589252, valid loss: 0.715526, valid acc: 0.508301\n",
      "\n",
      "epoch: 136, training loss: 0.666069, training acc: 0.589370, valid loss: 0.715589, valid acc: 0.508301\n",
      "\n",
      "epoch: 137, training loss: 0.665782, training acc: 0.589725, valid loss: 0.715654, valid acc: 0.509277\n",
      "\n",
      "epoch: 138, training loss: 0.665495, training acc: 0.590909, valid loss: 0.715719, valid acc: 0.509277\n",
      "\n",
      "epoch: 139, training loss: 0.665209, training acc: 0.591856, valid loss: 0.715785, valid acc: 0.507812\n",
      "\n",
      "epoch: 140, training loss: 0.664924, training acc: 0.591856, valid loss: 0.715852, valid acc: 0.508789\n",
      "\n",
      "epoch: 141, training loss: 0.664639, training acc: 0.592685, valid loss: 0.715920, valid acc: 0.508301\n",
      "\n",
      "epoch: 142, training loss: 0.664355, training acc: 0.593277, valid loss: 0.715989, valid acc: 0.508301\n",
      "\n",
      "epoch: 143, training loss: 0.664071, training acc: 0.594342, valid loss: 0.716059, valid acc: 0.507812\n",
      "\n",
      "epoch: 144, training loss: 0.663788, training acc: 0.594934, valid loss: 0.716130, valid acc: 0.508301\n",
      "\n",
      "epoch: 145, training loss: 0.663506, training acc: 0.595052, valid loss: 0.716202, valid acc: 0.509277\n",
      "\n",
      "epoch: 146, training loss: 0.663224, training acc: 0.595999, valid loss: 0.716275, valid acc: 0.509766\n",
      "\n",
      "epoch: 147, training loss: 0.662943, training acc: 0.596473, valid loss: 0.716349, valid acc: 0.510254\n",
      "\n",
      "epoch: 148, training loss: 0.662662, training acc: 0.597183, valid loss: 0.716424, valid acc: 0.510254\n",
      "\n",
      "epoch: 149, training loss: 0.662382, training acc: 0.598366, valid loss: 0.716500, valid acc: 0.510254\n",
      "\n",
      "epoch: 150, training loss: 0.662102, training acc: 0.598603, valid loss: 0.716577, valid acc: 0.509766\n",
      "\n",
      "epoch: 151, training loss: 0.661823, training acc: 0.598722, valid loss: 0.716654, valid acc: 0.510254\n",
      "\n",
      "epoch: 152, training loss: 0.661544, training acc: 0.599432, valid loss: 0.716733, valid acc: 0.510742\n",
      "\n",
      "epoch: 153, training loss: 0.661266, training acc: 0.599432, valid loss: 0.716812, valid acc: 0.510254\n",
      "\n",
      "epoch: 154, training loss: 0.660988, training acc: 0.599787, valid loss: 0.716893, valid acc: 0.509766\n",
      "\n",
      "epoch: 155, training loss: 0.660711, training acc: 0.600379, valid loss: 0.716974, valid acc: 0.510254\n",
      "\n",
      "epoch: 156, training loss: 0.660434, training acc: 0.601207, valid loss: 0.717057, valid acc: 0.510254\n",
      "\n",
      "epoch: 157, training loss: 0.660157, training acc: 0.601326, valid loss: 0.717140, valid acc: 0.510254\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 158, training loss: 0.659881, training acc: 0.601918, valid loss: 0.717224, valid acc: 0.509766\n",
      "\n",
      "epoch: 159, training loss: 0.659605, training acc: 0.602273, valid loss: 0.717309, valid acc: 0.508301\n",
      "\n",
      "epoch: 160, training loss: 0.659330, training acc: 0.602154, valid loss: 0.717395, valid acc: 0.508301\n",
      "\n",
      "epoch: 161, training loss: 0.659055, training acc: 0.602273, valid loss: 0.717482, valid acc: 0.508789\n",
      "\n",
      "epoch: 162, training loss: 0.658780, training acc: 0.602865, valid loss: 0.717569, valid acc: 0.508789\n",
      "\n",
      "epoch: 163, training loss: 0.658506, training acc: 0.603220, valid loss: 0.717658, valid acc: 0.508301\n",
      "\n",
      "epoch: 164, training loss: 0.658232, training acc: 0.604048, valid loss: 0.717747, valid acc: 0.509277\n",
      "\n",
      "epoch: 165, training loss: 0.657958, training acc: 0.605114, valid loss: 0.717838, valid acc: 0.509277\n",
      "\n",
      "epoch: 166, training loss: 0.657685, training acc: 0.605824, valid loss: 0.717929, valid acc: 0.509277\n",
      "\n",
      "epoch: 167, training loss: 0.657411, training acc: 0.606179, valid loss: 0.718021, valid acc: 0.508789\n",
      "\n",
      "epoch: 168, training loss: 0.657139, training acc: 0.607363, valid loss: 0.718114, valid acc: 0.508789\n",
      "\n",
      "epoch: 169, training loss: 0.656866, training acc: 0.607363, valid loss: 0.718208, valid acc: 0.507812\n",
      "\n",
      "epoch: 170, training loss: 0.656594, training acc: 0.607836, valid loss: 0.718303, valid acc: 0.508301\n",
      "\n",
      "epoch: 171, training loss: 0.656322, training acc: 0.608546, valid loss: 0.718399, valid acc: 0.509766\n",
      "\n",
      "epoch: 172, training loss: 0.656051, training acc: 0.608546, valid loss: 0.718496, valid acc: 0.510254\n",
      "\n",
      "epoch: 173, training loss: 0.655779, training acc: 0.608665, valid loss: 0.718593, valid acc: 0.510742\n",
      "\n",
      "epoch: 174, training loss: 0.655508, training acc: 0.609612, valid loss: 0.718691, valid acc: 0.510742\n",
      "\n",
      "epoch: 175, training loss: 0.655237, training acc: 0.609848, valid loss: 0.718790, valid acc: 0.511230\n",
      "\n",
      "epoch: 176, training loss: 0.654967, training acc: 0.610204, valid loss: 0.718890, valid acc: 0.511230\n",
      "\n",
      "epoch: 177, training loss: 0.654696, training acc: 0.610204, valid loss: 0.718991, valid acc: 0.510742\n",
      "\n",
      "epoch: 178, training loss: 0.654426, training acc: 0.610085, valid loss: 0.719093, valid acc: 0.510742\n",
      "\n",
      "epoch: 179, training loss: 0.654156, training acc: 0.610795, valid loss: 0.719196, valid acc: 0.510254\n",
      "\n",
      "epoch: 180, training loss: 0.653887, training acc: 0.611742, valid loss: 0.719299, valid acc: 0.509277\n",
      "\n",
      "epoch: 181, training loss: 0.653617, training acc: 0.612098, valid loss: 0.719403, valid acc: 0.509277\n",
      "\n",
      "epoch: 182, training loss: 0.653348, training acc: 0.612689, valid loss: 0.719509, valid acc: 0.508789\n",
      "\n",
      "epoch: 183, training loss: 0.653079, training acc: 0.613281, valid loss: 0.719615, valid acc: 0.508301\n",
      "\n",
      "epoch: 184, training loss: 0.652810, training acc: 0.613400, valid loss: 0.719721, valid acc: 0.508789\n",
      "\n",
      "epoch: 185, training loss: 0.652541, training acc: 0.614228, valid loss: 0.719829, valid acc: 0.509766\n",
      "\n",
      "epoch: 186, training loss: 0.652273, training acc: 0.614583, valid loss: 0.719938, valid acc: 0.510254\n",
      "\n",
      "epoch: 187, training loss: 0.652004, training acc: 0.614938, valid loss: 0.720047, valid acc: 0.510254\n",
      "\n",
      "epoch: 188, training loss: 0.651736, training acc: 0.615530, valid loss: 0.720157, valid acc: 0.510254\n",
      "\n",
      "epoch: 189, training loss: 0.651468, training acc: 0.615649, valid loss: 0.720268, valid acc: 0.509277\n",
      "\n",
      "epoch: 190, training loss: 0.651200, training acc: 0.615649, valid loss: 0.720380, valid acc: 0.508789\n",
      "\n",
      "epoch: 191, training loss: 0.650933, training acc: 0.616122, valid loss: 0.720492, valid acc: 0.508301\n",
      "\n",
      "epoch: 192, training loss: 0.650665, training acc: 0.616596, valid loss: 0.720606, valid acc: 0.507324\n",
      "\n",
      "epoch: 193, training loss: 0.650398, training acc: 0.616832, valid loss: 0.720720, valid acc: 0.506836\n",
      "\n",
      "epoch: 194, training loss: 0.650131, training acc: 0.616951, valid loss: 0.720835, valid acc: 0.506836\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-434d492677c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n\u001b[0;32m---> 12\u001b[0;31m                            feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 500, 0, 0, 0\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:%d\\n'%(EPOCH))\n",
    "        break\n",
    "        \n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    for i in range(0, (len(train_X) // batch_size) * batch_size, batch_size):\n",
    "        batch_x = str_idx(train_X[i:i+batch_size],dictionary,maxlen)\n",
    "        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n",
    "                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n",
    "        train_loss += loss\n",
    "        train_acc += acc\n",
    "    \n",
    "    for i in range(0, (len(test_X) // batch_size) * batch_size, batch_size):\n",
    "        batch_x = str_idx(test_X[i:i+batch_size],dictionary,maxlen)\n",
    "        acc, loss = sess.run([model.accuracy, model.cost], \n",
    "                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "    \n",
    "    train_loss /= (len(train_X) // batch_size)\n",
    "    train_acc /= (len(train_X) // batch_size)\n",
    "    test_loss /= (len(test_X) // batch_size)\n",
    "    test_acc /= (len(test_X) // batch_size)\n",
    "    \n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print('epoch: %d, pass acc: %f, current acc: %f'%(EPOCH,CURRENT_ACC, test_acc))\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "        \n",
    "    #print('time taken:', time.time()-lasttime)\n",
    "    print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'%(EPOCH,train_loss,\n",
    "                                                                                          train_acc,test_loss,\n",
    "                                                                                          test_acc))\n",
    "    EPOCH += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = sess.run(model.logits, feed_dict={model.X:str_idx(test_X,dictionary,maxlen)})\n",
    "print(metrics.classification_report(test_Y, np.argmax(logits,1), target_names = trainset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
